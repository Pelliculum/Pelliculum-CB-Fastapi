# üé¨ Fine-tuning et D√©ploiement d'un Chatbot de Recommandation de Films : Un Parcours d'Apprentissage

## üìö Table des mati√®res
1. [Introduction](#-introduction)
2. [Gen√®se et √âvolution du Projet](#-gen√®se-et-√©volution-du-projet)
3. [Choix Techniques et Dataset](#-choix-techniques-et-dataset)
4. [Choix des Technologies]()
5. [Processus It√©ratif de Fine-tuning](#-processus-it√©ratif-de-fine-tuning)
6. [D√©veloppement de l'API](#-d√©veloppement-de-lapi)
7. [Architecture et D√©ploiement](#-architecture-et-d√©ploiement)
8. [D√©ploiement sur HuggingFace Hub](#-d√©ploiement-sur-huggingface-hub)
9. [R√©sultats et Analyses](#-r√©sultats-et-analyses)
10. [Conclusion et Perspectives](#-conclusion-et-perspectives)

## üéØ Introduction

Ce projet s'inscrit dans le cadre d'un travail pratique sur le fine-tuning et le d√©ploiement d'un mod√®le de langage pour la recommandation de films. Notre objectif initial √©tait ambitieux : cr√©er un chatbot capable non seulement de sugg√©rer des films pertinents, mais aussi d'engager une conversation naturelle sur le cin√©ma, comprenant des requ√™tes complexes comme "j'ai aim√© ce film avec telle actrice, peux-tu me proposer un autre ?" ou "ce film m'a plu pour son ambiance, as-tu des suggestions similaires ?". Comme nous le verrons, la r√©alit√© du d√©veloppement nous a amen√©s √† revoir et adapter ces objectifs initiaux.

üìå **Acc√®s au Projet :**
- Interface web : https://chat.pelliculum.fr
- API : https://chat-api.pelliculum.fr
- Docs API : https://chat-api.pelliculum.fr/docs
- Code Frontend : https://github.com/Pelliculum/Pelliculum-CB-Front
- Mod√®le Hugging-Face : https://huggingface.co/RealDragonMA/Pelliculum-Chatbot

üöÄ **Installation et Lancement Local :**

**Frontend (Angular)**

```bash
# Pr√©requis
npm install -g @angular/cli

# Installation
git clone https://github.com/Pelliculum/Pelliculum-CB-Front
cd Pelliculum-CB-Front
npm install

# Lancement
npm run dev
```

**Backend (FastAPI)**

```bash
# Installation des d√©pendances
pip install fastapi uvicorn torch transformers requests

# Lancement
python main.py
# ou
uvicorn main:app --reload
```

L'API sera accessible sur http://localhost:8000 et le frontend sur http://localhost:4200.


## üå± Gen√®se et √âvolution du Projet

### üöÄ Premiers Pas et Difficult√©s Initiales

Notre parcours a d√©but√© avec une ambition peut-√™tre d√©mesur√©e. Nous voulions cr√©er un chatbot capable de comprendre et d'analyser de multiples aspects des films :
- Genres et sous-genres
- Acteurs et r√©alisateurs
- Budget et recettes
- Notes et critiques
- Ambiance et style

Cette approche s'est rapidement heurt√©e √† des obstacles majeurs. Le mod√®le produisait des r√©ponses totalement incoh√©rentes, allant jusqu'√† g√©n√©rer des recettes de p√¢tes au milieu d'une discussion sur le cin√©ma ! Ces premiers √©checs nous ont forc√©s √† remettre en question notre approche.

### üí° Intervention et Guidance du Professeur

Face √† ces difficult√©s, notre professeur est intervenu, ses conseils √©taient :

1. **Simplification de l'Objectif** :
   Commencez petit : Avant de vouloir faire un assistant conversationnel complet, il faut s'assurer que le mod√®le peut faire une t√¢che simple correctement.

2. **Focus sur les Donn√©es** :
   Il nous a √©galement aid√©s √† comprendre que la qualit√© du fine-tuning d√©pendait fortement de la clart√© et de la consistance des donn√©es d'entra√Ænement.

3. **Partage d'Expertise** :
   Il nous a fourni un exemple de code fonctionnel qui nous a servi de base pour la suite du projet.

## üîß Choix Techniques et Dataset

### ü§ñ S√©lection du Mod√®le

Le choix de **SmolLM2-135M-Instruct** repose sur plusieurs raisons :
- Taille adapt√©e aux ressources disponibles (pas de traitement d'images)
- Base d√©j√† instruite pour la compr√©hension du langage (anglais)
- Communaut√© active et documentation disponible

### üìä Analyse Approfondie du Dataset

Le dataset "wykonos/movies" a √©t√© s√©lectionn√© apr√®s une analyse d√©taill√©e de ses caract√©ristiques :

```plaintext
Statistiques cl√©s :
- 722,796 entr√©es
- 19 colonnes de m√©tadonn√©es
- Couverture temporelle extensive
```

#### Structure des Donn√©es :
```plaintext
id: identifiant unique (int64)
title: titre du film (string)
genres: liste des genres (string)
overview: r√©sum√© du film (string)
recommendations: liste d'IDs de films recommand√©s (string)
[...]
```

L'int√©gration des IDs TMDb s'est r√©v√©l√©e cruciale pour la suite du projet, nous permettant d'enrichir les r√©ponses du chatbot avec des informations d√©taill√©es et des visuels.

## üõ†Ô∏è Choix des Technologies
[![My Skills](https://skillicons.dev/icons?i=fastapi,python,angular,ts,docker,nginx,cloudflare)](https://skillicons.dev)

### Backend (API)
- **FastAPI** : Choisi pour :
  - Sa performance native gr√¢ce √† Starlette
  - Sa compatibilit√© excellente avec les mod√®les HuggingFace (via librairie transformers)
  - Sa documentation automatique (OpenAPI/Swagger)
  - Sa facilit√© d'int√©gration avec Pydantic pour la validation des donn√©es
  - Facilit√© du langage Python

### Frontend
- **Angular** : S√©lectionn√© pour :
  - Sa robustesse et sa maturit√©
  - Son syst√®me de composants r√©utilisables
  - Son excellent support TypeScript
  - Sa performance avec le change detection
  - Tout le groupe connait et sait utiliser Angular

### D√©ploiement
- **Docker** : Utilis√© pour :
  - La facilit√© de d√©ploiement

- **Nginx** : Choisi comme reverse proxy pour :
  - Facilit√© de mise en place
  - Sa gestion efficace du CORS
  - Son r√¥le de reverseproxy avec Docker

- **Cloudflare** : Utilis√© pour :
  - La gestion DNS
  - Le SSL gratuit

## üîÑ Processus It√©ratif de Fine-tuning

### ‚ùå Phase 1 : Les Premiers √âchecs

Notre premi√®re approche √©tait trop ambitieuse. Nous avons tent√© d'entra√Æner le mod√®le sur l'ensemble des m√©tadonn√©es :

```python
# Premier essai (trop ambitieux)
input_format = """
Titre: {title}
Genres: {genres}
R√©alisateur: {director}
Acteurs: {actors}
Budget: {budget}
Note: {rating}
"""
```

R√©sultats :
- Loss tr√®s √©lev√©e
- R√©ponses incoh√©rentes
- G√©n√©ration de contenu hors sujet (les fameuses recettes de p√¢tes !)

### ‚ú® Phase 2 : Simplification et Premiers Succ√®s

Sur conseil du professeur, nous avons drastiquement simplifi√© notre approche :

```python
# Approche simplifi√©e
PROMPT_TEMPLATE = """Suggest movies similar to {title}
movie recommendations:"""
```

Le but √©tait simplement que le mod√®le renvoit la colonne "recommendations" (liste d'id) du film √©nonc√© par l'utilisateur.

Nouveaux probl√®mes rencontr√©s :
1. Le mod√®le g√©n√©rait des IDs al√©atoires au lieu d'utiliser ceux du dataset
2. Quand nous avons remplac√© les IDs par les titres correspondants, il r√©pondait syst√©matiquement "None" avec une tr√®s bonne training loss (<0.01).
- Analyse du probl√®me :<br>
Le dataset contenait de nombreux films sans recommandations (valeur "None" dans la colonne recommendations)<br>
Durant l'entra√Ænement, le mod√®le a √©t√© expos√© √† de nombreux exemples o√π la r√©ponse attendue √©tait "None"<br>
Le mod√®le a donc appris que r√©pondre "None" √©tait une strat√©gie optimale pour minimiser la loss<br>
Techniquement, le mod√®le faisait exactement ce qu'on lui avait appris (d'o√π la bonne loss) mais ce n'√©tait pas le comportement souhait√©

- Solution :<br>
Filtrage du dataset pour ne garder que les films ayant des recommandations<br>
Transformation des IDs en titres de films<br>
Limitation √† 8 recommandations maximum par film<br>
V√©rification et nettoyage des donn√©es avant l'entra√Ænement

### üéØ Phase 3 : Raffinement et Stabilisation

Apr√®s 28 runs document√©s sur Weights & Biases, nous avons finalement trouv√© une configuration stable :

```python
# Configuration finale
lora_config = LoraConfig(
    r=64,
    lora_alpha=128,
    lora_dropout=0.05,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    bias="none",
    task_type="CAUSAL_LM"
)

training_args = SFTConfig(
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    warmup_steps=10,
    num_train_epochs=3,
    learning_rate=0.0002,
    # ...
)
```

! Ne pas prendre en compte les valeurs apr√®s l'√©tape 1000, nous utilisons le mod√®le de cette √©tape ! (le train a continu√© car nous n'avons pas sp√©cifi√© de max_steps dans les training_args, nous voulions voir la suite)

![WanDB train](/images/wandb_train.png)

### Pr√©paration Minutieuse des Donn√©es

Notre processus de pr√©traitement final incluait :
1. Filtrage des films sans recommandations
2. Conversion des IDs en titres
3. Limitation √† 8 recommandations maximum
4. V√©rification de la coh√©rence des donn√©es

```python
def replace_ids_with_titles(example):
    titles = []
    for movie_id in example["recommendations"].split('-'):
        if id_to_title.get(int(movie_id)):
            titles.append(id_to_title.get(int(movie_id)))
        if len(titles) == 8:
            break
    example["recommendations"] = ", ".join(titles) if titles else None
    return example
```


## ‚öôÔ∏è D√©veloppement de l'API

### üîå Architecture FastAPI et Int√©gration TMDb

L'API a √©t√© con√ßue pour enrichir les recommandations brutes du mod√®le. Voici le processus complet :

1. **üõ†Ô∏è R√©ception de la requ√™te** :
```python
@app.post("/recommend/")
async def get_recommendations(request: MovieRequest):
    messages = [
        {"role": "system", "content": "You are an expert in movie recommendation"},
        {"role": "user", "content": PROMPT_TEMPLATE.format(title=request.title)}
    ]
```

2. **G√©n√©ration et Post-traitement** :
```python
def search_tmdb_movies(title: str) -> Dict:
    """Recherche un film sur TMDb par titre."""
    url = f"{TMDB_BASE_URL}/search/movie"
    params = {
        "api_key": TMDB_API_KEY,
        "query": title,
        "language": "fr-FR",
        "include_adult": False
    }
    # ...
```

### Gestion des Cas Limites

L'API g√®re plusieurs sc√©narios :
- Films absents de TMDb
- Titres g√©n√©r√©s incorrectement par le mod√®le

## üèóÔ∏è Architecture et D√©ploiement

### üì° Infrastructure Technique

Notre infrastructure de d√©ploiement :

```plaintext
Architecture :
‚îî‚îÄ‚îÄ Serveur (4 cores, 4GB RAM)
    ‚îú‚îÄ‚îÄ Docker
    ‚îÇ   ‚îú‚îÄ‚îÄ API FastAPI
    ‚îÇ   ‚îî‚îÄ‚îÄ Front Angular
    ‚îú‚îÄ‚îÄ Nginx (Reverse Proxy)
    ‚îî‚îÄ‚îÄ Cloudflare DNS
```

![Architecture](/images/architecture.png)

### üíª Interface Utilisateur

L'interface utilisateur reprend les codes des chatbots modernes :
- Design √©pur√© inspir√© de ChatGPT
- Barre de saisie en bas de l'√©cran
- Affichage des recommandations avec posters et m√©tadonn√©es

![Accueil chat.pelliculum.fr](/images/chat_home.png)
![Suggest movies similar to Fast X](/images/chat_suggest_fast_x.png)
![Movie details](/images/chat_movie_details.png)

## üì¶ D√©ploiement sur HuggingFace Hub

### Push du Mod√®le
1. **Pr√©paration :**
   ```python
   from huggingface_hub import notebook_login
   notebook_login()
   ```
2. **Configuration :**
   ```python
   hub_model_id = "RealDragonMA/Pelliculum-Chatbot"
   ```
3. **Entra√Ænement avec Push :**
   ```python
   trainer = SFTTrainer(
      # ... autres configs ...
      args=SFTConfig(
         push_to_hub=True,
         hub_model_id=hub_model_id,
      )
   )
   ```
4. **Sauvegarde et Push :**
   ```python
   trainer.push_to_hub(dataset_name=dataset_name)
   ```

### Fusion et Optimisation
```python
# Fusion de l'adaptateur LoRA
model = model.merge_and_unload()

# Sauvegarde du mod√®le fusionn√©
output_merged_dir = os.path.join(OUTPUT_DIR, "final_merged_checkpoint")
model.save_pretrained(output_merged_dir, safe_serialization=True)

# Push du mod√®le fusionn√©
model.push_to_hub(hub_model_id)
```

### Inf√©rence via Text Generation

Notre mod√®le peut √™tre utilis√© via l'API HuggingFace ou directement avec la biblioth√®que transformers :

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("RealDragonMA/Pelliculum-Chatbot")
model = AutoModelForCausalLM.from_pretrained("RealDragonMA/Pelliculum-Chatbot")

# Configuration d'inf√©rence optimis√©e
model = model.eval()
inputs = tokenizer.encode(input_text, return_tensors="pt")
outputs = model.generate(
    inputs,
    max_new_tokens=50,
    temperature=0.2,
    top_p=0.45,
    do_sample=True
)
```

## üìä R√©sultats et Analyses

### üéØ Exemples Concrets

#### ‚úÖ Succ√®s
Requ√™te : "Suggest movies similar to Fast X"
```plaintext
R√©ponse : [Liste des films avec posters et m√©tadonn√©es]
```

![Suggest movies similar to Fast X](/images/chat_suggest_fast_x.png)

#### ‚ö†Ô∏è Limitations
1. **Temps de R√©ponse** : 
   - Moyenne de 8 secondes
   - Principalement d√ª aux requ√™tes TMDb

2. **Contraintes Linguistiques** :
   - N√©cessit√© de formuler les requ√™tes en anglais
   - Format sp√©cifique recommand√© : "Suggest movies similar to {title}"

3. **Limites du Dataset** :
   - Risque de recommandations al√©atoires pour les films absents du dataset
   - Pas de compr√©hension profonde des pr√©f√©rences utilisateur

## üéØ Conclusion et Perspectives

### üìù Le√ßons Apprises

1. **L'Importance de la Simplicit√©** :
   Notre parcours nous a appris qu'il vaut mieux exceller dans une t√¢che simple que d'√©chouer dans une t√¢che complexe.

2. **La Valeur de l'It√©ration** :
   Les 28 runs sur Weights & Biases t√©moignent de l'importance d'une approche it√©rative et persistante.

3. **Le R√¥le de l'Expertise** :
   L'intervention de notre professeur a √©t√© d√©cisive pour rediriger le projet vers une approche viable.

### üöÄ Pistes d'Am√©lioration

1. **Optimisation Technique** :
   - Mise en place d'un syst√®me de cache pour les requ√™tes TMDb
   - Optimisation du temps de r√©ponse du mod√®le

2. **Am√©lioration du Mod√®le** :
   - Extension du dataset d'entra√Ænement
   - Fine-tuning sur des conversations plus naturelles
   - Support multilingue

3. **Enrichissement Fonctionnel** :
   - Int√©gration de la compr√©hension des pr√©f√©rences utilisateur
   - Interface plus riche en fonctionnalit√©s

Ce projet, malgr√© ses limitations actuelles, d√©montre la possibilit√© de cr√©er un syst√®me de recommandation de films fonctionnel en utilisant des mod√®les de langage fine-tun√©s. Il illustre √©galement l'importance d'une approche progressive et it√©rative dans le d√©veloppement d'applications d'intelligence artificielle.